% chapter4.tex
\chapter{Order Book Events on a Poisson Network}
\label{chapter:four}

\section{Introduction}

\subsection{Summary}

\subsection{Contribution}

\section{Methodology}

\subsection{Model}
Order book dynamics are difficult to understand, but it is useful to think of two aspects of the trading environment: an external (endogenous) environment and an internal (endogenous) environment. Some orders are motivated by considerations that are external to the market as well as the information about the asset. Other orders are motivated by considerations based on external information about the asset. Still other orders are placed by traders that are indifferent to the external information?their orders only reflect the state of the market itself. The motivations are reflected in market microstructure models as liquidity traders, informed traders and market makers.

Limit order books are event-driven processes. In order to come up with a model that can serve as a simulator, we need a statistical framework that works in continuous-time; we need to be able to distinguish the order of events exactly. In addition, sequences of events generated by the simulator should reflect the actions of the user. In other words, the user should be able to change the expected outcome of the simulation, rather than simply learning the expectation. Point processes--and Poisson processes in particular--are statistical models that satifsy both criteria.

\subsubsection{Poisson Processes}
A \textit{Poisson process} is a continuous-time model that simulates a sequence of events $\{s_m\}_{m=1}^M$ occurring within a time interval $\left[0, T\right]$. The expected number of events that occur in any sub-interval $[\tau_1, \tau_2]$ is determined by the \textit{intensity} of the process, $\lambda(t)$. If the intensity is constant, then the expected number of events is $\lambda (\tau_2 - \tau_1)$. In general, the intensity is time-varying (e.g., in order to capture seasonalities in the frequency of events), in which case the expected number of events is equal to the \textit{integrated intensity},
\begin{equation}
	\mathcal{I}\left[t_1, t_2\right] = \int_{\tau_1}^{\tau_2} \lambda(t) \ dt = \mathbb{E} \left[ N \right]
\end{equation}

% Poisson processes have a long history of application in finance. They are used, for example, to model the distribution of the occurrence of rapid changes in stock prices--changes that take place over periods of time that are shorter than the frequency of the data--commonly referred to as ``jumps''. (Provide some additional applications here).

% A variant of the standard Poisson process is the \textit{self-exciting} Poisson process, or \textit{Hawkes process} \cite{}. The motivation behind the Hawkes process is simple: for some observable phenomena, the fact that an event occurs increases the probability the same event will occur in the near future. For example, gang-related crimes typically increase the probability that other crimes will occur because they are followed by a cycle of revenge crimes \cite{Cho2013}.

A \textit{self-exciting Poisson process}--or \textit{Hawkes process} (\cite{Hawkes1971})--is a type of inhomogeneous Poisson process. At any point in time, the intensity of a self-exciting Poisson process depends on the history of events preceding that point. This self-exciting feature allows the model to capture temporary increases in the probability of future events. For example, gang-related crimes increase the probability of subsequent gang-related crimes because they are followed by a cycle of revenge crimes (\cite{Cho2013}).

The intensity of a self-exciting Poisson process is given by
\begin{equation}
	\lambda(t) = \lambda^0(t) + \sum_{s_m < t} \phi(t - s_m),
\end{equation}
where $\phi$ is a non-negative \textit{impulse-response} function that produces an increase in the occurrence of events following an initial event. In the absence of events, the model functions as a standard Poisson process. When an event occurs, the model generates a momentary burst in the intensity of the event arrivals. If the process is \textit{stable}, it produces clusters of events. The process is \textit{unstable} if the feedback loop between events causes the intensity to explode.

Self-exciting Poisson processes can be simulated using the \textit{Poisson superposition principle} (\cite{Kingman1993}), which states that a collection of (independent) Poisson processes is equivalent to an individual Poisson process. More precisely, suppose that we are shown the event data from a collection of $K$ Poisson processes, $\{ \{ s_m^{(k)} \}_{m=1}^{M_k} \}_{k=1}^K$, but that the data have been merged into a single sequence of events without indicating their origins. Thus, we observe $\{ s_m \}_{m=1}^{\sum_k M_k}$, and there is no difference, from our perspective, between assuming that the data is generated by a collection of processes with intensities $\{\lambda_k(t)\}_{k=1}^K$ and assuming that it is generated by a solitary Poisson process with intensity $\lambda_{tot}(t) = \sum_{k=1}^K \lambda_k(t)$.

For a self-exciting Poisson process, the superposition principle implies a generative model in which each event is the offspring of either the ``background'' Poisson process, or of a Poisson process initiated by an earlier event. Therefore, the process can be simulated according to the following algorithm:
\begin{enumerate}
	\item Generate a sequence of events according to a standard Poisson process with intensity $\lambda_0(t)$ on the interval $[0,T]$.
	\item For each event $s_m \in [0, T]$, generate a sequence of events according to a Poisson process with intensity $\phi(t - s_m)$ on the interval $[s_m, T]$.
	\item Repeat Step 2 until no further events are occur.
\end{enumerate}

\subsubsection{Mutually Exciting Poisson Processes}
Self-exciting Poisson processes are easily extended to multiple processes. The multivariate model allows for connections between different types of events in addition to connections between events of the same type. Specifically, the intensity of the $n^{th}$ type of event is give by
\begin{equation} \label{eq:hawkes-intensity}
	\lambda_{n}(t) = \lambda_{n, 0}(t) + \sum_{s_m < t} h_{c_m, n}(t - s_m; \theta_{c_m, n})
\end{equation}

The generative model relies on a clever trick explained in \cite{Linderman2015}. Suppose that we had a collection of Poisson processes $\{\{s_{m_i}\}_{m_i = 1}^{M_i} \sim \mathcal{PP}(\lambda_i(t)) \ | \ i = 1, \dots, K\}$. The superposition principle asserts that the union of this collection is itself a Poisson process with parameter $\lambda(t) = \sum_{i=1}^{N} \lambda_i(t)$. The reverse is also true: if we know that the parameter of a Poisson process has the form $\lambda(t) = \sum_{i=1}^{N} \lambda_i(t)$, then the process is equivalent to a union of Poisson processes with parameters $\lambda_i(t)$. The functional form of the intensity parameter for the Hawkes process in equation \eqref{eq:hawkes-intensity} has exactly this form. Moreover, there is a one-to-one correspondence between the collection of Poisson processes implied by equation \eqref{eq:hawkes-intensity} and the events of the original process. In particular, each event can be interpreted as a Poisson process with intensity function
\begin{equation}
	\lambda_n^m(t) = \begin{cases}
				          0 & t \le s_m \\
				          h(t - s_m; \theta_{c_m, n}) & t > s_m.
				      \end{cases}
\end{equation}

(A figure that demonstrates a multivariate Hawkes process).


\subsubsection{Order Book Model}
Poisson networks provide a \textit{general} framework for modeling limit order book dynamics, but precisely how to use this framework depends on what aspect of limit order books we are interested in. We could use this framework to analyze the connections between limit order books: How does activity on order book A affect activity on order book B? Our objective is instead to construct a simulator of an individual limit order book without reference to external forces. We are interested in capturing the connection between different \textit{types} of orders and between orders affecting different \textit{levels} of the order book. Therefore the nodes of our model are pairs of \textit{type}-\textit{level} combinations. The types of orders that we consider are \textit{add}, \textit{delete}, and \textit{execute}. We are only interested in the first three levels of the order book, and so this results a graph with dimension $K = 3 \times 3 = 9$.

The dynamics of a limit order book are an event-driven process. Over longer periods of time the effects of individual events are smoothed out and it is reasonable, and practical, to model the book in terms of ``residual'' metrics: prices and shares. Over short time periods we need to model the book at the level of individual events, which is our motivation for using Poisson processes. But what are the classes of events that we should be interested in: what are the nodes of our graph? All orders can be described by a small set of properties. For example, in its out-going message data the NASDAQ exchange describes orders in terms of order type (add, delete, execute), side (bid, ask), price, number of shares, and a unique order reference number (used to match delete and execute orders with add orders). Certainly we should draw distinctions between order types and orders on different sides of the book. Prices are problematic because they can have a wide range of values, which would require us to create a large graph. More importantly, what is the connection between a bid add order at \$98.01 when the best bid is \$98.02, and a bid add order at \$98.01 when the best bid is \$98.10? Prices don't define meaningful classes of events, but the \textit{levels} of the order book that prices correspond to do. I define the level of an order to be the number of cents away from the best bid/ask that the order price corresponds to. So an add order at the current best bid price has level 0, and an add order one cent below the current best bid price has level 1. Add orders can also specify prices that improve upon the current best bid/ask, which we label as level -1 orders; delete and execute orders can never have level -1 (what about execution of ``iceberg" orders?). Sometimes a market order executes against multiple levels of the order book, which could be considered as two events occurring at the same time: an execute at the first level, and an execute at the second level. In order to account for this possibility we can define the level of an execute order as the maximum of the levels affected by the order. Execute orders are almost always level 1, but will be classified as level 2 when an order ``walks the book"\footnote{Report how often this occurs in your dataset.}. In order to keep the dimension of the graph tractable, we only consider add order events at levels -1, 0, and 1, an innocuous assumption given that orders outside of the top of the book don't play an important role in order book dynamics. Overall, this leaves us with 14 classes of events:
\begin{multline}
k =  \sum_{\substack{s \in \{bid,ask\}, \\ l \in \{-1, 0, 1\}}} \mathbb{I}(type=add, side=s, level=l) \\
+ \sum_{\substack{s \in \{bid,ask\}, \\ l \in \{0, 1\}}} \mathbb{I}(type=delete, side=s, level=l) \\
+ \sum_{\substack{s \in \{bid,ask\}, \\ l \in \{0, 1\}}} \mathbb{I}(type=execute, side=s, level=l).
\end{multline}
In some cases I find that \textit{no} market orders execute against the entire available volume at the best bid/ask, in which case I reduce the size of the graph by a further 2 classes.

The Poisson network model allows for a variety of dependencies between network nodes. A \textit{fully dependent} order model is a Poisson network with a fully-connected graph: the binary connection matrix uniformly one ($A = 1$). In a \textit{Bernoulli} order model, some nodes are connected, and some nodes are independent: whether two nodes are connected is determined by the flip of a coin, and the same coin is used for each pair of nodes in the network (we could consider more complicated strucutres in which different coins are used for each pair of nodes). In our model of the limit order book we assume that every node is connected to every other node. Assuming that the network is fully-connected simplifies inference as we only need to worry about the strength between nodes ($W$), which reduces the computational complexity of the Gibbs sampling algorithm.


\subsection{Inference}
Estimation of Hawkes processes in the finance literature relies on maximum-likelihood methods (e.g., \cite{Bowsher2007}, \cite{Large2007}, and \cite{Bacry2013}). I introduce a Bayesian inference procedure to the finance literature. The method is based on work in the field of computational neuroscience, first published in \cite{Linderman2015}.

\subsubsection{Modelling Choices}
The model framework is the network Poisson process model from the previous section. We assume that $\lambda_0(t) = \lambda$.

(Write down assumptions about the impulse-response functional form. Why this form? What types of shapes can it take?)

\subsubsection{Gibbs Sampling Algorithm}
We can sample the conditional posterior distributions of the model parameters using conjugate priors and Markov Chain Monte Carlo methods \cite{}.

(\textit{Weights}) If the weights have gamma priors,
\begin{equation}
	w_{n,n'} \sim \Gamma(\kappa, \nu_{n,n'}),
\end{equation}
then their conditional distributions are also gamma:
\begin{equation}
	p(w_{n,n'} | \{s_m, c_m, \omega_m\}_{m=1}^M, a_{n,n'} = 1, \kappa, \nu_{n,n'}) = \Gamma(\tilde{\kappa}_{n,n'}, \tilde{\nu}_{n,n'}),
\end{equation}
where
\begin{align}
	\tilde{\kappa}_{n,n'} &= \kappa + M_{n,n'}, \\
	\tilde{\nu}_{n,n'} &= \nu_{n,n'} + M_n,
\end{align}
and
\begin{align}
	M_n &= \sum_{m=1}^{M} \mathbb{I}\left[ c_m = n \right], \\
	M_{n,n'} &= \sum_{m=1}^M \mathbb{I}\left[ c_m = n \right] \mathbb{I}\left[ c_{m'} = n' \right] \mathbb{I}\left[ \omega_{m'} = m \right].
\end{align}
The first sufficient statistic ($M_n$) is the number events occuring on node $n$. The second sufficient statistics ($M_{n,n'}$) is the number of events on node $n'$ caused by events on node $n$.

(\textit{Impulses}) The likelihood is conjugate with a normal-gamma distribution. If the priors of the impulse parameters are normal-gamma,
\begin{equation}
	(\mu_{n,n'}, \tau_{n,n'}) \sim \mathcal{NG} (\mu_\mu, \kappa_\mu, \alpha_\tau, \beta_\tau),
\end{equation}
then the conditional posterior distributions are normal-gamma with parameters
\begin{align}
	\tilde{\mu}_{n,n'} &= \frac{\kappa_\mu \mu_\mu + M_{n,n'}\bar{x}_{n,n'}}{\kappa_\mu + M_{n,n'}}, \\
	\tilde{\kappa}_{n,n'} &= \kappa_{\mu} + M_{n,n'}, \\
	\tilde{\alpha}_{n,n'} &= \alpha_\tau + \frac{M_{n,n'}}{2}, \\
	\tilde{\beta}_{n,n'} &= \frac{\nu_{n,n'}}{2} + \frac{M_{n,n'} \kappa_\mu (\bar{x}_{n,n'} - \mu_\mu)^2}{2(M_{n,n'} + \kappa_\mu)},
\end{align}
The sufficients statistics in this case are defined with respect to
\begin{equation}
	x_{m,m'} = \ln \left( \frac{s_{m'} - s_m}{\Delta t_{max} - (s_{m'} - s_m)} \right),
\end{equation}
which is the log of the ratio of the time elapsed since event $m$ occured to the time remaining until event $m$ can no longer cause another event. The sufficient statistics are the mean and variance of $x_{m,m'}$:
\begin{align}
	\bar{x}_{n,n'} &= \frac{1}{M_{n,n'}} \sum_m \sum_{m'} \mathbb{I}\left[ c_m = n \right] \mathbb{I}\left[ c_{m'} = n' \right] \mathbb{I}\left[ \omega_{m'} = m \right] x_{m,m'}, \\
	\bar{\nu}_{n,n'} &= \frac{1}{M_{n,n'}} \sum_m \sum_{m'} \mathbb{I}\left[ c_m = n \right] \mathbb{I}\left[ c_{m'} = n' \right] \mathbb{I}\left[ \omega_{m'} = m \right] (x_{m,m'} - \bar{x}_{n,n'})^2
\end{align}

(\textit{Biases}) If the background rates have gamma priors,
\begin{equation}
	\lambda_n^{0} \sim \Gamma(\alpha_n^0, \beta_n^0),
\end{equation}
then their conditional distributions are also gamma:
\begin{equation}
	p(\lambda_n^{0} | \{s_m, c_m, \omega_m\}_{m=1}^M, \alpha_n^0, \beta_n^0) \sim \Gamma(\tilde{\alpha}_n^0, \tilde{\beta}_n^0),
\end{equation}
where
\begin{align}
	\tilde{\alpha}_n^0 &= \alpha_n^0 + \sum_{m=1}^M \mathbb{I}(c_m=n) \mathbb{I}(\omega_m=0) = \alpha_n^0 + M_n^0, \\
	\tilde{\beta}_0^n &= \beta_0 + T.
\end{align}
In this case the sufficient statistics are the sample time ($T$) and the number of node $n$ events induced by the background rate.

(\textit{Parents}) The conditional distribution of the parent indicator variable is a multinomial with probability mass function
\begin{equation} \label{eq: parents_posterior}
	p(\omega_m = n \ | \ s_m, \{ \lambda_n(t)\}_{n=1}^M) = \frac{\lambda_n(s_m)}{\lambda_{c_m}^0 + \sum_{n'=1}^{m - 1} \lambda_{n'}(s_m)},
\end{equation}
for $n = 0, \dots, m - 1$, where
\begin{equation}
	\lambda_{n'}(s_m) = w_{c_{n'},c_m} \cdot \hbar(s_m - s_{n'}; \theta_{c_m,c_n'})
\end{equation}

This says that the probability that the parent of the $m^{th}$ event is the $n^{th}$ event equals the relative contribution of the $n^{th}$ event to the aggregate intensity at the time of $m^{th}$ event. As a result, the most likely parent at any time is the event that is \textit{most active} at that time. The conditional distributions are independent, so the most likely parents can be identified in parallel. We can also see from the definition of $\lambda_{n'}(s_m)$ that the most likely parent of an event is not, in general, the most recent event: the likelihood depends on the \textit{strength} of the connection between between the event node and the potential parent event node. In addition, events are sometimes caused by the background rate, which is reflected in the inclusion of $n=0$ in equation \eqref{eq: parents_posterior}.


\section{Data}

\section{Results}
What does the model reveal about limit order book dynamics in general? This section analyzes estimates of the event-driven model for a broad (by microstructure standards) cross-section of stocks. For each stock, I perform MCMC sampling for the network Poisson model with the same hyperparameter selection and settings described in previous sections using a single day of message data. The point-estimator considered is the posterior median, which minimizes posterior absolute loss risk.

All stocks belong to the S\&P 500, with computational considerations determining the specific selection. I collected message data for all stocks in the S\&P 500 and used this data to identify order book events according to the definitions described above. The MCMC algorithm employed scales on the order of the number of events squared. To reduce the total sampling time, the sample excludes stocks with more than 60,000 observations between 10:30 am and 3:00 pm. An alternative approach would be to select a random subsample of manageable size from the skipped stocks. In fact, it makes more sense to use the same number of events for each stock, rather than the same amount of time, as it is this number that determines the posterior distribution, and message activity varies considerably across stocks.

The model assumes a constant background rate of events, which is an unrealistic assumption. More likely, the background rate varies over time as market conditions change. It is possible, and not computationally costly, to add a piecewise constant or linear background rate to the model. On the other hand, time-varying background rates are only generally useful if they capture seasonalities, and are complicated to analyze. As a compromise, I summarize each stock day by a single background rate parameter but restrict each day of message data to stable trading hours (10:30 am to 3:00 pm).

\subsection{Connections}
What types of interactions characterize aggregate limit order book dynamics? Figure \ref{fig:weights_full_sample} presents the median of the connection weight matrices. Two connections stand out immediately: add orders at the best bid following trades that increase the best ask, and add orders at the best ask following trades the decrease the best bid. This type of behavior has been noted elsewhere and shows that trading algorithms recognize these trades as dependable signals of underlying supply and demand. In fact, just to the right of these connections are the connections between bids that exhaust the best prices and subsequent add orders that improve the offer price on the opposite side of the book. (Likely what is happening is that this connection activates first, and then new orders queue at the newly established best price—we could confirm this by looking at the impulse responses of the two connections).

The opposite side of the connection matrix displays a related phenomenon between executions and subsequent deletions. Here, price-changing trades are seen to have two substantial effects: one on the same side of the book, and one on the opposite side. The result is the same on both sides: traders tend to delete orders at the best price. This activity is consistent with the connections between executions and additions. Traders on the opposite side of the book need to remove liquidity that will lose priority if an incoming order establishes a new price on that side. Traders on the same side of the book will cancel their orders if they anticipate that the price will continue to move in that direction (although there is no substantial connection to add orders on this side of the book at the new best price, so traders do not appear to re-establish their prior positions).

Finally, the median raw estimates demonstrate strong links between price-improving add orders and subsequent add orders at the new best prices (on the same side). Price-improving orders are significant order book events that traders respond to by supplying fresh liquidity to preserve execution priority.

All of the connections addressed thus far have been identified by looking at the raw estimates of connection weights. These raw estimates represent the expected number of child events following parent events, so they have a direct economic meaning. Alternatively, we might ask which connections are most important relative to background activity. That is, which interactions contribute the most to the child event’s intensity. To do so, we simple normalize all connection weights by their child weight background rates. This normalization gives a somewhat different interpretation of the most important connections.

The pair of links that stands out the most is those between price-improving add orders and price-deteriorating delete orders. Thus, the model identifies the main driving force behind such delete orders: they are a response to price-improving orders on the same side of the book. In fact, “response” may not be the right description: these orders are likely to come from the same trading algorithm posted an add order above the best bid, and then immediately deleting that order if it fails to execute. (By looking at the impulse response on this connection we can get a better idea of the typical amount of time traders are willing to wait for an execution). The value of these weights is around 120, which means that the expected number of such delete orders following a price-improving add order is 120 times greater than the expected number of such delete order generated as “background” activity. Furthermore, the reverse link becomes clear after normalization: price-deteriorating deletions precede price-improving orders on the same side of the book. These two pairs of connections form a loop that produces cycles of “flickering” quotes: quick, repeated additions and deletions of price-improving liquidity.

Another pair of connections that appears much more clearly under this normalization is that between executions and subsequent executions. It is well-known that trades are “clustered” in time. As trades are relatively rare events, this clustering is less prominent in the raw weight estimates. After normalization, we recover the observation that executions “generate” additional executions on the same side of the book. According to the model estimates, clustering accounts for around 60 times as many executions at the top of the book as exogenous executions at the best offer, and approximately 40 times as many for price-reducing trades.

Finally, it is interesting to note the order of importance between the execute-add connections reverses under this normalization: price-deteriorating trades have a more substantial impact on the probability of observing price-improving orders on the opposite side of the book than they do on the likelihood of add orders at the top of the book.

\subsection{Impulse Responses}
Beyond the strength of connections between events, it is necessary to discuss the timescale of these interactions. The median impulse response is the impulse response function evaluated at the posterior median point-estimator of the relevant parameters. Figure \ref{fig:impulse_full_sample} plots the median impulse responses of the four pairs of connections discussed in the previous section.

One thing to note is that, in addition to the magnitude, the timing of the connections is symmetric in all four cases.

Figure \ref{fig:impulse_full_sample} shows the delay in response to price-deteriorating trades of both add orders at the opposite-side best price, and that improve the best offer on the opposite side of the book. Both reactions peak between one microsecond (1/100000) and one millisecond (1/1000), where the bulk of their mass concentrates, and of the four connections shown, they are the fastest. The impulse response of deletions to price-deteriorating executions takes place on a longer timescale, with a maximum intensity attained around one millisecond, while the intensity of delete orders following price-improving add orders peaks between one millisecond and one second. Thus, so-called "flickering orders" are in fact permitted to sit for a relatively extended period before being canceled.

It is worth briefly contrasting these results with results based on exponential impulse response functions. An exponential impulse response function creates an immediate increase in the intensity of child events. In contrast, with a logarithmic-normal impulse response, there is a lag between the time of a parent event and the time it generates a “spike” in the child event intensity. At timescales on the order of minutes, the distinction is unimportant. As demonstrated by Figure \ref{fig:impulse_full_sample}, however, interactions between order book events take place on timescales measured in milliseconds: on this timescale the difference is meaningful.

\subsection{Cross-Section}
The size of the sample analyzed in this study presents a unique opportunity to investigate the cross-section of order book dynamics. A question of particular interest is whether we can meaningfully categorize order books beyond the aggregate point estimates presented above. For example, if order book dynamics determined by simple order book characteristics (e.g., the bid-ask spread), then we might expect to find distinct clusters of order books. I explore this possibility through an analysis of point estimates of the weight matrices. I observe little evidence of clusters of orders book dynamics.

Clustering refers to a form of unsupervised learning in which we attempt to find groups of objects that are more similar to each other than they are to objects that belong to different groups. The objects may or may not belong to defined groups, but ideally, a clustering algorithm would be able to identify the correct group of examples without access to labels. For example, presented with a sequence of hand-written digits, a clustering algorithm should be able to learn that there are ten types of objects (and also label the images correctly).

In the present study, we wish to cluster 12 by 12 matrices representing the connections between order book events. In principle, a suitable method for learning the underlying structure of the data would involve training a few layers of a convolutional neural network to extract potential “features” of the weight matrices. Convolutional neural networks achieve state-of-the-art performance on image recognition tasks, but also rely on large datasets. With a sample size of just 401 stocks, and with each observation having a dimension of 144 features, such an approach is infeasible. Instead, I pursue a less elaborate, linear method based on principal component analysis: instead of attempting to cluster the data in its original space, I project the data onto a low-dimensional space determined by principal component analysis and try to groups in the low-dimensional data.

Figure \ref{fig:pca_chapter4} displays the results. The top panel of the figure presents the first and second principal components for reference; the bottom panel displays the proportion of the variance explained by the top twelve principal components as well as a scatterplot of the projection of each connection weight matrix onto the subspace spanned by the first two principal components.

\subsection{Likelihood \& Stability}
The features identified by the model thus far are encouraging, but do they explain the data well? Next, I briefly examine the issues of model fit and stability. Log-likelihood measures model fit. More precisely, I compare the log-likelihood of model estimates to a standard (homogeneous) Poisson process with a constant intensity vector, which I refer to as the “baseline” model. The maximum of the absolute values of the eigenvalues of the connection matrix determines the stability of the model. Models with stability value higher than one are unstable: with positive probability, they generate an infinite number of events in any finite amount of time. At the other extreme, a model with a stability value of zero is a homogeneous Poisson process.

Figure \ref{fig:distributions} compares the fit of the network model versus the baseline model. For each stock, the point-estimator of the network model is the approximate posterior median based on the MCMC sampling algorithm outlined above. Setting the weight matrix to zero and computing the approximate median of the posterior distribution by direct sampling produces a Bayesian point-estimator of the baseline model. I compute the values in the histogram shown as follows. For each stock, I randomly select 100 subsamples consisting of 180 events each from the same dataset used to construct the point estimates. On each of the subsamples, I compute the ratio of the log-likelihood of the network and baseline models divided by the number of events in the sample and record the average of this value across subsamples. The resulting histogram shows that the network model improves on the baseline model by approximately 4 "bits per event"; the minimum improvement is 1.5 bits per events, and the maximum exceeds 7. These results quantify the extent to which a complex interaction of events determines order book dynamics.

The range of values in Figure \ref{fig:distributions} demonstrates that the network model fits some stocks better than others. Figure \ref{fig:likelihood_scatter} compares the fit of the model to characteristics of each stock’s order book. Specifically, for each stock, I use the same message data used to identify order book events to calculate four attributes of the message data. The characteristics are defined as follows: $Volume$ is the total number of shares traded as identified by execution messages, including executions against hidden orders; $Price$ is the average midprice across all order book updates; $Volatility$ is the standard deviation of $Price$; $Spread$ is the average bid-ask spread across all order book updates. The scatterplots in Figure \ref{fig:likelihood_scatter} demonstrate that the quality of the network model fit depends on these characteristics. The model appears to work better for stocks with higher trade volumes, lower average prices, lower price volatility, and smaller bid-ask spreads.

Confirming model stability is essential for two reasons. First, because the network model is only physically meaningful if it is stable. Second, because an unstable model cannot serve as a useful order book simulator. Figure \ref{fig:distributions} presents a histogram of the stability values based on median point estimates. The median value is approximately 0.85, which is well below both the instability threshold (1.0) and estimates reported in studies of related models (e.g., REF). Two of the models have unstable dynamics according to this measure. Figure \ref{fig:stability_scatter} shows scatterplots of stability versus the microstructure characteristics from Figure \ref{fig:likelihood_scatter}. Stability appears to decrease with lower trading volume but is otherwise uncorrelated with the remaining variables. Overall, these results demonstrate that the model learns parameter values leading to highly inhomogeneous, but stable, dynamics.

The results thus far demonstrate that including interactions between events improves model fit relative to a baseline model. They also show that some interaction terms are stronger than others, in both an absolute and relative sense. We can further analyze the importance of interactions terms by answering the following question: “How much does the model fit change if I turn off the weaker connections?” I answer this question by performing a simple experiment. For a given stock I select a set of threshold values based on the distribution of the estimated matrix weights. Under the minimum threshold, the model includes nearly all of the of the interaction terms, while at the maximum the model reduces to the baseline homogeneous Poisson process. For each threshold, I measure the in-sample and out-of-sample likelihood using subsamples as described above.

Figures \ref{fig:threshold_LLY} and \ref{fig:threshold_SIG} demonstrate the results for two randomly selected stocks (LLY and SIG). The plots display similar patterns: including weights whose logarithm is less than -4—-which amounts to ignoring around half the interaction terms--has little or no effect on the fit of the model. In fact, retaining weights above 0.1 (about a quarter of the interaction terms) achieves the majority of the benefit of the network structure. These observations provide a new perspective on Figure \ref{fig:weights_full_sample}. Figure \ref{fig:weights_threshold} recreates the plot of the median connection matrix, but this time applies a hard threshold of -2 to the logarithm of the raw weights. The interaction terms that survive are those that were identified earlier, meaning that these are in fact the essential elements of typical order book dynamics.


\section{Conclusion}

% fig:weights_full_sample
\begin{sidewaysfigure}[ht!]
\small
\linespread{1}
\centering
\captionsetup{labelsep=colon, font=footnotesize, justification=centerfirst, width=\linewidth}
\caption{Median Event Connections}
\label{fig:weights_full_sample}
\includegraphics[width=\textwidth]{chapter4/weights_full_sample}
\captionsetup{position=below, font=footnotesize, justification=justified, width=\linewidth}
\caption*{Note: The figure shows the median connection strength between event types across the full sample of stocks. Larger values indicate stronger connections, which are expected to generate a greater number of child events. (Left) The median of connection weight estimates using the raw weight matrix estimates. This figure identifies the most important connections in an absolute sense. (Right) The median connection weight matrix after normalizing parent-child weights by the estimated background rate of child events. This figure identifies the most important connections in a relative sense.}
\end{sidewaysfigure}

% fig:impulse_full_sample
\begin{figure}[ht!]
\small
\linespread{1}
\centering
\captionsetup{labelsep=colon, font=footnotesize, justification=centerfirst, width=\linewidth}
\caption{Median Impulse Responses}
\label{fig:impulse_full_sample}
\includegraphics[width=\textwidth]{chapter4/impulse_full_sample}
\captionsetup{position=below, font=footnotesize, justification=justified, width=\linewidth}
\caption*{Note: The figure shows the median impulse response for selected parent-child event connection pairs across the full sample. The horizontal axis is logarithmic time to improve interoperability. The figure demonstrates that important connections operate at timescales from one microsecond to one second, that estimates of connections pairs are typically symmetric, and that the model identifies the response to parents as a localized "spike" in the probability of child events.}
\end{figure}

% fig:distributions
\begin{figure}[ht!]
\small
\linespread{1}
\centering
\captionsetup{labelsep=colon, font=footnotesize, justification=centerfirst, width=\linewidth}
\caption{Distribution of Likelihood \& Stability}
\label{fig:distributions}
\includegraphics[width=\textwidth]{chapter4/distributions}
\captionsetup{position=below, font=footnotesize, justification=justified, width=\linewidth}
\caption*{Note: The figure shows histograms of the likelihood and stability of model estimates across the full sample. To compute the likelihood, I select random subsamples from the full sample of daily events on which the model was estimated and average the likelihood across subsamples. The values shown are the differences between the inhomogeneous and homogeneous model divided by the total number of events in each subsample. Stability is computed as the absolute value of the largest eigenvalues of the connection weight matrix. Values below one are stable; values at and above one are unstable.  The figure demonstrates that the network model produces a better in-sample fit to the data and that the model estimates are stable in almost all cases.}
\end{figure}

% fig:likelihood_scatter
\begin{figure}[ht!]
\small
\linespread{1}
\centering
\captionsetup{labelsep=colon, font=footnotesize, justification=centerfirst, width=\linewidth}
\caption{Likelihood vs. Order Book Characteristics}
\label{fig:likelihood_scatter}
\includegraphics[width=\textwidth]{chapter4/likelihood_scatter}
\captionsetup{position=below, font=footnotesize, justification=justified, width=\linewidth}
\caption*{Note: The figure shows scatterplots of the likelihood of estimated models versus properties of the order books calculated from the message data used to identified order book events (i.e., the same day and hours). Volume is the total number of shares traded, including executions against hidden orders; Price is the average midprice across order book updates; Volatility is the standard deviation of Price; Spread is the average bid-ask spread across order book updates. The likelihood is the difference in likelihood between the network model and a standard Poisson model divided by the number of events.}
\end{figure}

% fig:stability_scatter
\begin{figure}[ht!]
\small
\linespread{1}
\centering
\captionsetup{labelsep=colon, font=footnotesize, justification=centerfirst, width=\linewidth}
\caption{Stability vs. Order Book Characteristics}
\label{fig:stability_scatter}
\includegraphics[width=\textwidth]{chapter4/stability_scatter}
\captionsetup{position=below, font=footnotesize, justification=justified, width=\linewidth}
\caption*{Note: The figure shows scatterplots of the stability of estimated models versus properties of the order books calculated from the message data used to identified order book events (i.e., the same day and hours). Volume is the total number of shares traded, including executions against hidden orders; Price is the average midprice across order book updates; Volatility is the standard deviation of Price; Spread is the average bid-ask spread across order book updates. Stability is the maximum absolute value of the estimated connection weight matrix.}
\end{figure}

% fig:threshold_LLY
\begin{figure}[ht!]
\small
\linespread{1}
\centering
\captionsetup{labelsep=colon, font=footnotesize, justification=centerfirst, width=\linewidth}
\caption{Threshold Analysis (LLY)}
\label{fig:threshold_LLY}
\includegraphics[width=\textwidth]{chapter4/threshold_LLY}
\captionsetup{position=below, font=footnotesize, justification=justified, width=\linewidth}
\caption*{Note: The figure shows the improvement in the likelihood of the network model compared to a standard Poisson model for various thresholds levels. (Top) For each threshold level, the likelihood improvement is computed using the estimated connection weights with all weights below the threshold set to zero. The blue curve shows results using events from the same day the model is estimated on; the orange curve shows results computed on the following day. (Below) A histogram of the connection weights. The figure demonstrates that the ability of the model to fit the data is largely invariant to weights below the median estimated weight and that the model fits out-of-sample data as well as in-sample data.}
\end{figure}

% fig:threshold_SIG
\begin{figure}[ht!]
\small
\linespread{1}
\centering
\captionsetup{labelsep=colon, font=footnotesize, justification=centerfirst, width=\linewidth}
\caption{Threshold Analysis (SIG)}
\label{fig:threshold_SIG}
\includegraphics[width=\textwidth]{chapter4/threshold_SIG}
\captionsetup{position=below, font=footnotesize, justification=justified, width=\linewidth}
\caption*{Note: The figure shows the improvement in the likelihood of the network model compared to a standard Poisson model for various thresholds levels. (Top) For each threshold level, the likelihood improvement is computed using the estimated connection weights with all weights below the threshold set to zero. The blue curve shows results using events from the same day the model is estimated on; the orange curve shows results computed on the following day. (Below) A histogram of the connection weights. The figure demonstrates that the ability of the model to fit the data is largely invariant to weights below the median estimated weight and that the model fits out-of-sample data as well as in-sample data.}
\end{figure}

% fig:weights_threshold
\begin{figure}[ht!]
\small
\linespread{1}
\centering
\captionsetup{labelsep=colon, font=footnotesize, justification=centerfirst, width=\linewidth}
\caption{Composite Weights with Threshold}
\label{fig:weights_threshold}
\includegraphics[width=\textwidth]{chapter4/weights_threshold}
\captionsetup{position=below, font=footnotesize, justification=justified, width=\linewidth}
\caption*{Note:}
\end{figure}

% fig:pca
\begin{sidewaysfigure}[ht!]
\small
\linespread{1}
\centering
\captionsetup{labelsep=colon, font=footnotesize, justification=centerfirst, width=\linewidth}
\caption{Principal Component Analysis}
\label{fig:pca_chapter4}
\includegraphics[width=\textwidth]{chapter4/pca}
\captionsetup{position=below, font=footnotesize, justification=justified, width=\linewidth}
\caption*{Note:}
\end{sidewaysfigure}

% fig:eigenvalues
\begin{figure}[ht!]
\small
\linespread{1}
\centering
\captionsetup{labelsep=colon, font=footnotesize, justification=centerfirst, width=\linewidth}
\caption{Principal Component Analysis (continued)}
\label{fig:eigenvalues_chapter4}
\includegraphics[width=\textwidth]{chapter4/eigenvalues}
\captionsetup{position=below, font=footnotesize, justification=justified, width=\linewidth}
\caption*{Note:}
\end{figure}
